#MNIST (Modified National Institute of Standards and Technology dataset)

First steps to understand the inner workings of deep learning were taken using the MNIST database.

The database contains 60 000 handwritten numbers 0-9.

In the 1980’s Yann le Cun became interested in using a computer to read handwritten numbers.  During this time he was a post-doctoral student with Geoffrey Hinton in Toronto. In 1998 he reduced the error rate on the MNIST database to 0.7% using a convolutional neural network. (LeNet)

In a convolutional network we use filters to determine the edges and shapes of objects in images producing feature maps. Then pooling is used to reduce the inputs.

20 years later I stood on the shoulders of these giants. What a view!

A pixel in an image is represented by a number between 0 and 255. Pixels can be represented by a tensor(vector).  Multidimensional tensors are very similar to an array or a matrix.  

Computer’s vision is made possible by changing pixels into tensors.

Stochastic Gradient Descent (SGD) is the heart of deep learning. Principles were taught by Jeremy Howard on a mini-MNIST database distinguishing between 3’s and 7’s. This was a fascinating learning experience.

SGD is done with an optimizer:

	Assigning random weights and biases to the input (the w and b in y=wx+b).
	Activation of neurons using sigmoid or relu (y>0).
	Calculate the loss (target(label) – prediction(y)).

	Backpropagate:
	Determine gradient of the loss and parameters with calculus.
	Recalculate w and b by changing the parameters with a fraction of calculated gradient (learning rate) using mini batches (stochastic).

	Repeat

Back propagation to improve neural networks was already described by Paul Werbos in 1974 in his PhD thesis at Harvard.


Then I used the full MNIST database for practice.

The database already has “training” and “testing” files. Therefore I used a GrandparentSplitter to produce the train and valid files for the Dataloaders.
splitter = GrandparentSplitter(train=”training”, valid=” testing”)

Resnet18 was used for the architecture of the learner.

Learner was trained with 5 epochs and fit_one_cycle with maximum learning rate of 0.1.

Accuracy of 98.8% was reached with these hyperparameters.

Inference was a bit of a headache.

I could not get learn_inf.predict(img) to work. Getting the image for inference in the correct format  I think was the problem.

In the end I loaded all the nines in the training dataset in a dataloader:

dl = learn.dls.test_dl(nines)
predictions, y = learn.get_preds(dl=dl)

I randomly selected predictions[10] and the algorithm predicted a 74% probability for 9, 11% for a 4 and 9% for a 5. Not bad if you look at the image:
 
<img width="81" alt="image" src="https://user-images.githubusercontent.com/67821144/126980185-835f26ac-afa6-4dea-ba64-44d9c905f450.png">

