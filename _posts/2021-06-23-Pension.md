#Pension

Early retirement at 58 years January 2020. After 24 years at Kimberley Hospital in South Africa as a pediatrician it was time for a change. Travel and a hike to basecamp Annapurna in Nepal was planned. Then a change in career, maybe research.

March 2020 Covid-19 struck.

A change of plans needed. Contract at local hospital May-December 2020 for half day.

Still a need for a change. Seen a TED talk a few years ago by Jeremy Howard on machine learning. Must be the future and I want to be a part of it.

March-November 2020: Coursera Python courses from University of Michigan with Dr. Chuck Severance the main contributor. With no previous coding experience, it was indeed a steep learning curve. Learned the value of You Tube, Google and forums to solve problems.

December 2020 – January 2021: Mathematics of Machine Learning from Imperial College of London. (Coursera)

With no calculus and matrices experience the learning curve steepened. The worth of YouTube and forums increased. Knowledge regarding vectors, matrixes and the math behind a gradient descent grew.

February 2021: Start with Fastai slowly. The video lectures by Jeremy Howard and Rachel Thomas are fascinating but I struggled with the practical part of the course.

It took me 3 weeks to master a virtual GPU on Colab or Gradient. In the end I settle for Gradient at Paperspace. Working mainly in the morning to beat the American rush hour that starts 6 hours later. It took another 2 weeks to figure out how the storage and uploading of files work.

Steep learning curves are not upsetting anymore and every time something works it produces enough dopamine (feel good hormone) to encourage persistence or tenacity.

Again YouTube, Google and forums are my assistants.

I have worked through the first 8 lessons. I have classified lions and tigers after images were searched via Bing but failed building an app.  I classified handwritten numbers from the MNIST database. Connecting to Kaggle was a bit of a nightmare with Colab yielding better results than Gradient but I managed to access the bulldozer auction data. 

I did the ‘Hello World’ of Kaggle: The Titanic database. Uploading the files to Gradient was easier than trying to access data via Kaggle API. My best position improved to top 17% with a Random Forest plot setting only two hyperparameters, only slightly better than using the default hyperparameters. Using GridSearch to try to identify better hyperparameters did not yield improved results.

I have built models for a Hospital Stay and a Solar Generation database found on Kaggle. Random Forest working quite well. Testing neural networks for Tabular every time. Took a while to figure out how to do predictions for neural networks.

With a bit more confidence it is time to redo the Fastai course. Starting with a BabyClassifier using photos from Google Search.  Classifier must differentiate between a happy and a sad baby. Can then perhaps expand later to ill babies. Finding photos of ill babies could be a challenge. This is a possible bigger project for later.

It is time to pick a project and start a blog.
